{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.color'], mpl.rcParams['axes.labelcolor'], mpl.rcParams['xtick.color'], mpl.rcParams['ytick.color'] = ['white']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClf:\n",
    "    def __init__(self, algo='gaussian', alpha=None):\n",
    "        '''\n",
    "        algo:str - 'gaussian': For normal distribution, Used for classification.\n",
    "                   'multinomial': For multinomially distributed data\n",
    "                   'bernoulli': used when feature vectors are binary\n",
    "                   'complement': Adaptation of the standard Multinomial Naive Bayes (MNB) algorithm \n",
    "                       that is particularly suited for imbalanced data sets wherein the algorithm \n",
    "                       uses statistics from the complement of each class to compute the modelâ€™s weight.\n",
    "        alpha:float - for 'gaussian' algorithm, alpha is the \n",
    "                        for 'multinomial' algorithm, if alpha=1, Laplace smoothing\n",
    "                                        elif alpha < 1, Lidstone smoothing\n",
    "                                        else alpha >= 0, prevents zero probabilities\n",
    "        '''\n",
    "        if algo == 'gaussian':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1e-9\n",
    "            self.alpha = alpha\n",
    "        elif algo == 'multinomial':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1.0\n",
    "            self.alpha = alpha\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''\n",
    "        X - (n_datapoints, n_features)\n",
    "        y - (n_datapoints, 1)\n",
    "        '''\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        n_datapoints, n_features = X.shape\n",
    "        assert len(y) == n_datapoints\n",
    "        y = y.reshape(-1)\n",
    "        # Frequency Table / Prior probabilities\n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        priors = np.zeros((n_classes, 1))\n",
    "        mu = np.zeros((n_classes, n_features))\n",
    "        sigma2 = np.zeros((n_classes, n_features))\n",
    "        # ith class features will be stored in ith index\n",
    "        features_by_class = []\n",
    "        for i in range(n_classes):\n",
    "            features_by_class.append(X[y==classes[i], :])\n",
    "            priors[i] = len(features_by_class[-1]) / n_datapoints\n",
    "            mu[i] = features_by_class[-1].mean(axis=0)\n",
    "            sigma2[i] = features_by_class[-1].var(axis=0)\n",
    "        self.n_classes = n_classes\n",
    "        self.priors = priors\n",
    "        self.mu = mu\n",
    "        self.sigma2 = sigma2 + self.alpha * np.var(X, axis=0).max()    # To avoid numerical error (from sklearn source)\n",
    "    \n",
    "    def predict(self, X, return_probs=False):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if self.algo == 'gaussian':\n",
    "            cond_prob = np.zeros((self.n_classes, X.shape[0]))\n",
    "            for i in range(self.n_classes):\n",
    "                cond_prob[i, :] = (np.exp(-((X-self.mu[i])**2 / (2 * self.sigma2[i]))) / np.sqrt(2*np.pi*self.sigma2[i])).prod(axis=1)\n",
    "            probs = self.priors * cond_prob\n",
    "            yp = probs.argmax(axis=0)\n",
    "        elif self.algo == 'multinomial':\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if return_probs:\n",
    "            return yp, probs\n",
    "        return yp\n",
    "    \n",
    "    @staticmethod\n",
    "    def score(y, y_pred):\n",
    "        return 100 * (y==y_pred).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB:\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='gaussian')\n",
    "class BernoulliNB:\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='bernoulli')\n",
    "class MultinomialNB:\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='multinomial')\n",
    "class ComplementNB:\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='complement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with sklearn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to compare and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, plot_confusion_matrix\n",
    "def pipe(X, y, models, feature=None, fit_prior=True, normalize=False, norm_method=\"fro\", alpha=1, test_size=0.3):\n",
    "#     plt.figure(figsize=(10,15))\n",
    "#     plot_X = X\n",
    "#     if X.ndim == 2:\n",
    "#         if X.shape[1] > 1:\n",
    "#             if feature is None:\n",
    "#                 print(\"Supported only for 1 feature, ignoring other features for plotting\")\n",
    "#                 plot_X = X[:, 0].reshape(-1, 1)\n",
    "#             else:\n",
    "#                 plot_X = X[:, feature].reshape(-1, 1)\n",
    "#     plt.scatter(plot_X, y, color='black')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    for k in models:\n",
    "        print(k)\n",
    "        if 'sk' in k:\n",
    "            models[k].fit(X_train, y_train)\n",
    "        else:\n",
    "            if normalize:\n",
    "                X_train = Normalizer(method=norm_method).normalize(X_train)\n",
    "                X_test = Normalizer(method=norm_method).normalize(X_test)\n",
    "            models[k].train(X_train, y_train)\n",
    "\n",
    "        y_pred = models[k].predict(X)\n",
    "        print(\"-\"*20, k, \"-\"*20)\n",
    "        print(\"sklearn Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", accuracy_score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", accuracy_score(y, y_pred))\n",
    "        print(\"My Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", NaiveBayesClf().score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", NaiveBayesClf().score(y, y_pred))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 5, 3, 2, 2, 1, 5, 5, 1, 3]).reshape(-1, 1)\n",
    "y = np.array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skGNB\n",
      "-------------------- skGNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.007518796992481203\n",
      "Whole dataset Accuracy score =  0.47058823529411764\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  0.7518796992481203\n",
      "Whole dataset Accuracy score =  47.05882352941177\n",
      "myGNB\n",
      "-------------------- myGNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.0\n",
      "Whole dataset Accuracy score =  0.0\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  0.0\n",
      "Whole dataset Accuracy score =  0.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as sknb\n",
    "models = {\n",
    "    'skGNB': sknb.GaussianNB(),\n",
    "    'myGNB': NaiveBayesClf(algo='gaussian')\n",
    "}\n",
    "models = pipe(X, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       ...,\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['skGNB'].sigma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       ...,\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12],\n",
       "       [2.46267984e-12, 2.46267984e-12, 2.46267984e-12, ...,\n",
       "        2.46267984e-12, 2.46267984e-12, 2.46267984e-12]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['myGNB'].sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1eaed580cf7c29f044a9e517f1cd4a7dd69c4b1f\" style=\"background-color: white\">\n",
    "\n",
    "## Gaussian naive Bayes\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/685339e22f57b18d804f2e0a9c507421da59e2ab\" style=\"background-color: white\">\n",
    "\n",
    "## Multinomial naive Bayes\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0bf7b8e6edae7359863977eca136558ce5b68568\" style=\"background-color: white\">\n",
    "<pre>Many conditional probabilities are multiplied, one for each feature. This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable\n",
    "</pre>\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e53303c0644c3d64e5eb86210023e76198285e0c\" style=\"background-color: white\">\n",
    "\n",
    "## Bernoulli naive Bayes\n",
    "<img src=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
