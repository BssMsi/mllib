{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''\n",
    "    method:['gd', 'cd', 'bfgs', 'lbfgs', 'liblinear'] - https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "    https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451\n",
    "    alpha:float - if method is 'gd': Learning rate in case of Gradient descent\n",
    "                  if method is 'cd' and regularization is 'elastic-net': Elastic-net alpha parameter\n",
    "    regularization:[None, 'l1', 'l2'] - None: No regularization,\n",
    "                                        'l1': L1 penalty = absolute sum of the weights\n",
    "                                        'l2': L2 penalty = sum of squares of the weights\n",
    "    fit_intercept:bool - To use the intercept term, if False, X is assumed to be centered\n",
    "    penalty:float - a.k.a lambda, multiplier that controls \n",
    "    n_iters:int - number of iterations in case of Gradient Descent Method\n",
    "    '''\n",
    "    def __init__(self, method='gd', alpha=0.1, fit_intercept=False, regularization='l2', reg_intercept=False, penalty=1.0, n_iters=400):\n",
    "        if regularization is None:\n",
    "            self.reg = None\n",
    "        else:\n",
    "            regularization = regularization.lower()\n",
    "            if regularization == 'l1' or regularization == 'lasso':\n",
    "                self.reg = 'l1'\n",
    "            elif regularization == 'l2' or regularization == 'ridge':\n",
    "                self.reg = 'l2'\n",
    "        method = method.lower()\n",
    "        if method == 'gd':\n",
    "            assert self.reg == 'l2' or self.reg is None, \"Gradient descent method can only be used with L2 regularization or None\"\n",
    "        self.method = method\n",
    "        self.penalty = penalty\n",
    "        self.reg_intercept = reg_intercept\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.W = np.array([])\n",
    "        self.n_iters = n_iters\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1+np.exp(-z))\n",
    "        \n",
    "    def train(self, X, y, return_history=False):\n",
    "        '''\n",
    "        X - (n_datapoints, n_features)\n",
    "        y - (n_datapoints, 1)\n",
    "        '''\n",
    "        if self.fit_intercept:\n",
    "            # To accomodate the bias / y-intercept term\n",
    "            X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "        n_datapoints, n_features = X.shape\n",
    "        assert y.size == n_datapoints, \"X and y must have same number of rows\"\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "        n_classes = len(np.unique(y))\n",
    "        if n_classes > 2:\n",
    "            self.mode = 'ovr'\n",
    "            W = np.zeros((n_features, n_classes))\n",
    "            cost = []\n",
    "            # One vs rest strategy\n",
    "            for c in range(n_classes):\n",
    "                y_c = np.where(y == c, 1, 0)\n",
    "                if return_history:\n",
    "                    W_c, cost_c = self._algo(X, y_c, True)\n",
    "                    W[:, c] = W_c.ravel()\n",
    "                    cost.append(cost_c)\n",
    "                else:\n",
    "                    W[:, c] = self._algo(X, y_c, False).ravel()\n",
    "        else:\n",
    "            self.mode = 'binary'\n",
    "            if return_history:\n",
    "                W, cost = self._algo(X, y, True)\n",
    "            else:\n",
    "                W = self._algo(X, y, False)\n",
    "        self.W = W\n",
    "        if return_history:\n",
    "            return cost\n",
    "        return self\n",
    "\n",
    "    def _algo(self, X, y, cost_history):\n",
    "        n_datapoints, n_features = X.shape\n",
    "        W = np.zeros((n_features, 1))\n",
    "        cost = []\n",
    "        # Using Gradient Descent\n",
    "        if self.method == 'gd':\n",
    "            if self.reg is None:\n",
    "                if cost_history:\n",
    "                    for i in range(self.n_iters):\n",
    "                        h = self.sigmoid(X@W)\n",
    "                        W -= self.alpha * (X.T @ (h - y)) / n_datapoints\n",
    "                        cost.append((- y.T@np.log(h) - (1-y.T)@np.log(1-h)).item() / n_datapoints)\n",
    "                else:  \n",
    "                    for i in range(self.n_iters):\n",
    "                        W -= self.alpha * (X.T @ (self.sigmoid(X@W) - y)) / n_datapoints\n",
    "            elif self.reg == 'l1':\n",
    "                print(\"Gradient Descent not implemented for L1\")\n",
    "                raise NotImplementedError\n",
    "            elif self.reg == 'l2':\n",
    "                if self.fit_intercept and not(self.reg_intercept):\n",
    "                    if cost_history:\n",
    "                        for i in range(self.n_iters):\n",
    "                            h = self.sigmoid(X@W)\n",
    "                            W[0] = W[0] - self.alpha * (X[:, 0] @ (h - y)) / n_datapoints\n",
    "                            W[1:] = W[1:] - self.alpha * (X[:, 1:].T @ (h - y) + self.penalty * W[1:].sum()) / n_datapoints\n",
    "                            cost.append((- y.T@np.log(h) - (1-y.T)@np.log(1-h) + self.penalty * (W**2).sum() / 2 ).item() / n_datapoints)\n",
    "                    else:\n",
    "                        for i in range(self.n_iters):\n",
    "                            h = self.sigmoid(X@W)\n",
    "                            W[0] = W[0] - self.alpha * (X[:, 0] @ (h - y)) / n_datapoints\n",
    "                            W[1:] = W[1:] - self.alpha * (X[:, 1:].T @ (h - y) + self.penalty * W[1:].sum()) / n_datapoints\n",
    "                else:\n",
    "                    if cost_history:\n",
    "                        for i in range(self.n_iters):\n",
    "                            W = W - self.alpha * (X.T @ (self.sigmoid(X@W) - y) + self.penalty * W.sum()) / n_datapoints\n",
    "                            cost.append((- y.T@np.log(h) - (1-y.T)@np.log(1-h) + self.penalty * (W**2).sum() / 2 ).item() / n_datapoints)\n",
    "                    else:\n",
    "                        for i in range(self.n_iters):\n",
    "                            W = W - self.alpha * (X.T @ (self.sigmoid(X@W) - y) + self.penalty * W.sum()) / n_datapoints\n",
    "        else:\n",
    "            print(\"Unknown method, might not be implemented yet, training failed\")\n",
    "            raise NotImplementedError\n",
    "        if cost_history:\n",
    "            return W, cost\n",
    "        return W\n",
    "        \n",
    "    def predict(self, X, return_prob=False, threshold=0.5):\n",
    "        if self.fit_intercept:\n",
    "            X = np.insert(X, 0, np.ones(X.shape[0]), axis=1)\n",
    "        prob = self.sigmoid(X@self.W)\n",
    "        if self.mode == 'ovr':\n",
    "            pred = prob.argmax(axis=1).ravel()\n",
    "        elif self.mode == 'binary':\n",
    "            pred = np.where(prob > threshold, 1, 0).ravel()\n",
    "        else:\n",
    "            raise ValueError\n",
    "        if return_prob:\n",
    "            return pred, np.around(prob, decimals=2)\n",
    "        return pred\n",
    "\n",
    "    @staticmethod\n",
    "    def score(y, y_pred):\n",
    "        return 100 * (y==y_pred).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, plot_confusion_matrix\n",
    "def pipe(X, y, models, normalize=False, test_size=0.2):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, shuffle=True, random_state=3)\n",
    "    for k in models:\n",
    "        if 'sk' in k:\n",
    "            models[k].fit(X_train, y_train)\n",
    "        else:\n",
    "            if normalize:\n",
    "                X_train = Normalizer(method=norm_method).normalize(X_train)\n",
    "                X_test = Normalizer(method=norm_method).normalize(X_test)\n",
    "            models[k].train(X_train, y_train)\n",
    "\n",
    "        y_pred = models[k].predict(X)\n",
    "        print(\"-\"*20, k, \"-\"*20)\n",
    "        print(\"sklearn Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", accuracy_score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", accuracy_score(y, y_pred))\n",
    "        print(\"My Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", LogisticRegression().score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", LogisticRegression().score(y, y_pred))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2, deg=6, fit_intercept=False):\n",
    "    if hasattr(X1, \"__iter__\"):\n",
    "        out = np.zeros((len(X1), deg))\n",
    "    else:\n",
    "        out = np.zeros((1, deg))\n",
    "    for i in range(1, deg+1):\n",
    "        for j in range(i+1):\n",
    "            out[:, i-1] = X1**(i-j) * X2**j\n",
    "    if fit_intercept:\n",
    "        return np.insert(out, 0, np.ones(out.shape[0]), axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- skLR --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9\n",
      "Whole dataset Accuracy score =  0.9666666666666667\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  90.0\n",
      "Whole dataset Accuracy score =  96.66666666666667\n",
      "-------------------- myLR --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9666666666666667\n",
      "Whole dataset Accuracy score =  0.9666666666666667\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  96.66666666666667\n",
      "Whole dataset Accuracy score =  96.66666666666667\n",
      "-------------------- myLR_reg_int --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9333333333333333\n",
      "Whole dataset Accuracy score =  0.96\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  93.33333333333333\n",
      "Whole dataset Accuracy score =  96.0\n",
      "-------------------- skLR_no_int --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9666666666666667\n",
      "Whole dataset Accuracy score =  0.9733333333333334\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  96.66666666666667\n",
      "Whole dataset Accuracy score =  97.33333333333333\n",
      "-------------------- myLR_no_int --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9666666666666667\n",
      "Whole dataset Accuracy score =  0.96\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  96.66666666666667\n",
      "Whole dataset Accuracy score =  96.0\n"
     ]
    }
   ],
   "source": [
    "alpha = .1\n",
    "pen = 1.0\n",
    "reg = 'l2'\n",
    "models = {\n",
    "    'skLR': linear_model.LogisticRegression(C=pen, penalty=str(reg).lower(), fit_intercept=True),\n",
    "    'myLR': LogisticRegression(alpha=alpha, regularization=reg, fit_intercept=True, penalty=pen),\n",
    "    'myLR_reg_int': LogisticRegression(alpha=alpha, fit_intercept=True, regularization=reg, reg_intercept=True, penalty=pen),\n",
    "    'skLR_no_int': linear_model.LogisticRegression(penalty=str(reg).lower(), fit_intercept=False, C=pen),\n",
    "    'myLR_no_int': LogisticRegression(alpha=alpha, regularization=reg, fit_intercept=False, penalty=pen),\n",
    "}\n",
    "models = pipe(X, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
