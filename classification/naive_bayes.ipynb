{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['text.color'], mpl.rcParams['axes.labelcolor'], mpl.rcParams['xtick.color'], mpl.rcParams['ytick.color'] = ['white']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, algo='gaussian', alpha=None):\n",
    "        '''\n",
    "        algo:str - 'gaussian': For normal distribution, Used for classification.\n",
    "                   'multinomial': For multinomially distributed data\n",
    "                   'bernoulli': used when feature vectors are binary\n",
    "                   'complement': Adaptation of the standard Multinomial Naive Bayes (MNB) algorithm \n",
    "                       that is particularly suited for imbalanced data sets wherein the algorithm \n",
    "                       uses statistics from the complement of each class to compute the modelâ€™s weight.\n",
    "        alpha:float - for 'gaussian' algorithm, alpha is the \n",
    "                      for 'multinomial' algorithm, if alpha=1, Laplace smoothing\n",
    "                                        elif alpha < 1, Lidstone smoothing\n",
    "                                        else alpha >= 0, prevents zero probabilities\n",
    "        '''\n",
    "        if algo == 'gaussian':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1e-9\n",
    "            self.alpha = alpha\n",
    "        elif algo == 'bernoulli':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1.0\n",
    "            self.alpha = alpha\n",
    "        elif algo == 'multinomial':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1.0\n",
    "            self.alpha = alpha\n",
    "        elif algo == 'complement':\n",
    "            self.algo = algo\n",
    "            if alpha is None:\n",
    "                alpha = 1.0\n",
    "            self.alpha = alpha            \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def train(self, X, y):\n",
    "        '''\n",
    "        X - (n_datapoints, n_features)\n",
    "        y - (n_datapoints, 1)\n",
    "        '''\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        n_datapoints, n_features = X.shape\n",
    "        assert len(y) == n_datapoints\n",
    "        y = y.reshape(-1)\n",
    "        \n",
    "        classes = np.unique(y)\n",
    "        n_classes = len(classes)\n",
    "        priors = np.zeros((n_classes, 1))\n",
    "        \n",
    "        if self.algo == 'gaussian':\n",
    "            mu = np.zeros((n_classes, n_features))\n",
    "            sigma2 = np.zeros((n_classes, n_features))\n",
    "            for i in range(n_classes):\n",
    "                features_by_class = X[y==classes[i], :]\n",
    "                priors[i] = len(features_by_class) / n_datapoints\n",
    "                mu[i] = features_by_class.mean(axis=0)\n",
    "                sigma2[i] = features_by_class.var(axis=0)\n",
    "            self.mu = mu\n",
    "            self.sigma2 = sigma2 + self.alpha * np.var(X, axis=0).max()    # To avoid numerical error (from sklearn source)\n",
    "        elif self.algo == 'multinomial' or self.algo == 'bernoulli':\n",
    "            likelihood = np.zeros((n_classes, n_features))\n",
    "            for i in range(n_classes):\n",
    "                features_by_class = X[y==classes[i], :]\n",
    "                priors[i] = len(features_by_class) / n_datapoints\n",
    "                temp = features_by_class.sum(axis=0)+self.alpha\n",
    "                likelihood[i, :] = temp / temp.sum()\n",
    "            self.likelihood = likelihood\n",
    "        elif self.algo == 'complement':\n",
    "            theta = np.zeros((n_classes, n_features))\n",
    "            for i in range(n_classes):\n",
    "                temp = self.alpha + X[y!=classes[i], :].sum(axis=0)\n",
    "                theta[i, :] = temp / temp.sum()\n",
    "            w = np.log(theta)\n",
    "            w = w / np.abs(w).sum()\n",
    "            self.w = w\n",
    "        self.n_classes = n_classes\n",
    "        self.priors = priors\n",
    "    \n",
    "    def predict(self, X, return_probs=False):\n",
    "        if X.ndim < 2:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if self.algo == 'gaussian':\n",
    "            cond_prob = np.zeros((self.n_classes, X.shape[0]))\n",
    "            for i in range(self.n_classes):\n",
    "                cond_prob[i, :] = (np.exp(-((X-self.mu[i])**2 / (2 * self.sigma2[i]))) / np.sqrt(2*np.pi*self.sigma2[i])).prod(axis=1)\n",
    "            probs = self.priors * cond_prob\n",
    "            yp = probs.argmax(axis=0)\n",
    "        elif self.algo == 'bernoulli':\n",
    "            cond_prob = np.zeros((self.n_classes, X.shape[0]))\n",
    "            for i in range(self.n_classes):\n",
    "                cond_prob[i, :] = (np.power(self.likelihood[i], X) * np.power(1-self.likelihood[i], 1-X)).prod(axis=1)\n",
    "            probs = self.priors * cond_prob\n",
    "            yp = probs.argmax(axis=0)\n",
    "        elif self.algo == 'multinomial':\n",
    "            cond_prob = np.zeros((self.n_classes, X.shape[0]))\n",
    "            for i in range(self.n_classes):\n",
    "                cond_prob[i, :] = X @ np.log(self.likelihood[i])\n",
    "            probs = np.log(self.priors) + cond_prob\n",
    "            yp = probs.argmax(axis=0)\n",
    "        elif self.algo == 'complement':\n",
    "            probs = self.w @ X.T\n",
    "            yp = probs.argmin(axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if return_probs:\n",
    "            return yp, probs\n",
    "        return yp\n",
    "    \n",
    "    @staticmethod\n",
    "    def score(y, y_pred):\n",
    "        return 100 * (y==y_pred).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(NaiveBayes):\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='gaussian')\n",
    "class BernoulliNB(NaiveBayes):\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='bernoulli')\n",
    "class MultinomialNB(NaiveBayes):\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='multinomial')\n",
    "class ComplementNB(NaiveBayes):\n",
    "    def __init__(self, ):\n",
    "        super().__init__(algo='complement')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing with sklearn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to compare and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, plot_confusion_matrix\n",
    "def pipe(X, y, models, feature=None, fit_prior=True, normalize=False, norm_method=\"fro\", alpha=1, test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    for k in models:\n",
    "        print(k)\n",
    "        if 'sk' in k:\n",
    "            models[k].fit(X_train, y_train)\n",
    "        else:\n",
    "            if normalize:\n",
    "                X_train = Normalizer(method=norm_method).normalize(X_train)\n",
    "                X_test = Normalizer(method=norm_method).normalize(X_test)\n",
    "            models[k].train(X_train, y_train)\n",
    "\n",
    "        y_pred = models[k].predict(X)\n",
    "        print(\"-\"*20, k, \"-\"*20)\n",
    "        print(\"sklearn Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", accuracy_score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", accuracy_score(y, y_pred))\n",
    "        print(\"My Accuracy scores ---------\")\n",
    "        print(\"Test Accuracy score = \", NaiveBayes().score(y_test, models[k].predict(X_test)))\n",
    "        print(\"Whole dataset Accuracy score = \", NaiveBayes().score(y, y_pred))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1, 2, 5, 3, 2, 2, 1, 5, 5, 1, 3]).reshape(-1, 1)\n",
    "y = np.array([1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "# Doesn't work, need to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skGNB\n",
      "-------------------- skGNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9555555555555556\n",
      "Whole dataset Accuracy score =  0.9533333333333334\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  95.55555555555556\n",
      "Whole dataset Accuracy score =  95.33333333333333\n",
      "myGNB\n",
      "-------------------- myGNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.9555555555555556\n",
      "Whole dataset Accuracy score =  0.9533333333333334\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  95.55555555555556\n",
      "Whole dataset Accuracy score =  95.33333333333333\n",
      "skBNB\n",
      "-------------------- skBNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.28888888888888886\n",
      "Whole dataset Accuracy score =  0.3333333333333333\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  28.88888888888889\n",
      "Whole dataset Accuracy score =  33.333333333333336\n",
      "myBNB\n",
      "-------------------- myBNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.8222222222222222\n",
      "Whole dataset Accuracy score =  0.84\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  82.22222222222223\n",
      "Whole dataset Accuracy score =  84.0\n",
      "skMNB\n",
      "-------------------- skMNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.8888888888888888\n",
      "Whole dataset Accuracy score =  0.9333333333333333\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  88.88888888888889\n",
      "Whole dataset Accuracy score =  93.33333333333333\n",
      "myMNB\n",
      "-------------------- myMNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.8888888888888888\n",
      "Whole dataset Accuracy score =  0.9333333333333333\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  88.88888888888889\n",
      "Whole dataset Accuracy score =  93.33333333333333\n",
      "skCNB\n",
      "-------------------- skCNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.6666666666666666\n",
      "Whole dataset Accuracy score =  0.6666666666666666\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  66.66666666666667\n",
      "Whole dataset Accuracy score =  66.66666666666667\n",
      "myCNB\n",
      "-------------------- myCNB --------------------\n",
      "sklearn Accuracy scores ---------\n",
      "Test Accuracy score =  0.6666666666666666\n",
      "Whole dataset Accuracy score =  0.6666666666666666\n",
      "My Accuracy scores ---------\n",
      "Test Accuracy score =  66.66666666666667\n",
      "Whole dataset Accuracy score =  66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as sknb\n",
    "models = {\n",
    "    'skGNB': sknb.GaussianNB(),\n",
    "    'myGNB': NaiveBayes(algo='gaussian'),\n",
    "    'skBNB': sknb.BernoulliNB(),\n",
    "    'myBNB': BernoulliNB(),\n",
    "    'skMNB': sknb.MultinomialNB(),\n",
    "    'myMNB': MultinomialNB(),\n",
    "    'skCNB': sknb.ComplementNB(),\n",
    "    'myCNB': ComplementNB()\n",
    "}\n",
    "models = pipe(X, y, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Predict formula\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1eaed580cf7c29f044a9e517f1cd4a7dd69c4b1f\" style=\"background-color: white\">\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/685339e22f57b18d804f2e0a9c507421da59e2ab\" style=\"background-color: white\">\n",
    "\n",
    "## Bernoulli Naive Bayes\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/2b23b8affe1fa31b1ce499d5d2944d9763ff2e6e\" style=\"background-color: white\">\n",
    "\n",
    "## Multinomial Naive Bayes\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e4f747edf8743902562d2bc725d01c7ac3ef7a63\" style=\"background-color: white\"> Used in sklearn\n",
    "<pre>Many conditional probabilities are multiplied, one for each feature. This can result in a floating point underflow. It is therefore better to perform the computation by adding logarithms of probabilities instead of multiplying probabilities. The class with the highest log probability score is still the most probable\n",
    "</pre>\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e53303c0644c3d64e5eb86210023e76198285e0c\" style=\"background-color: white\">\n",
    "\n",
    "## Complement Naive Bayes\n",
    "<img src=\"nb_imgs/CNB_theta_w.png\" style=\"background-color: white\">\n",
    "<img src=\"nb_imgs/CNB_predict.png\" style=\"background-color: white\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
